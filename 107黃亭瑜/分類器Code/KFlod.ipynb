{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title = ['Unigram+All','Logistic Regression','Random Forest','KNN','SVM','Decision Tree','Naive Bayes']\n",
    "row1 = ['Accuracy']\n",
    "row2 = ['Precision']\n",
    "row3 = ['Recall']\n",
    "row4 = ['Micro']\n",
    "row5 = ['Macro']\n",
    "row6 = ['Weighted']\n",
    "row7 = ['AUC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram = pd.read_csv('OrignalData/Model_all/Unigram_MS.csv',encoding=\"UTF-8\") \n",
    "data = pd.read_excel('OrignalData/Model_all/Data_MS.xlsx',encoding=\"UTF-8\") \n",
    "answer = pd.read_csv('OrignalData/Model_all/Answer_MS.csv',encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.merge(unigram, data, on='Doc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_result = result.values\n",
    "y_answer = answer.values\n",
    "X = X_result[:,1:]\n",
    "y = y_answer[:].ravel() \n",
    "# scoring = ['accuracy', 'precision', 'recall', 'f1_micro', 'f1_macro', 'f1_weighted', 'roc_auc'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.793815539189\n",
      "0.510233529872\n",
      "0.434311839672\n",
      "0.793815539189\n",
      "0.648123655811\n",
      "0.780752652275\n",
      "0.775166601052\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "acc = []\n",
    "f1 = []\n",
    "f2 = []\n",
    "f3 = []\n",
    "roc_auc = []\n",
    "p =[]\n",
    "r = []\n",
    "\n",
    "kf = KFold(n_splits = 10)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "#     print(\" %s %s \" % (train_index, test_index))\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred = np.array(lr.predict(X_test))\n",
    "    y_score = np.array(lr.predict_proba(X_test)[:,1])\n",
    "    y_test = np.array(y_test) #y_true\n",
    "    acc.append(lr.score(X_test,y_test))\n",
    "    f1.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    f2.append(f1_score(y_test, y_pred, average='micro'))\n",
    "    f3.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    roc_auc.append(roc_auc_score(y_test, y_score))\n",
    "    p.append(precision_score(y_test, y_pred))\n",
    "    r.append(recall_score(y_test, y_pred))\n",
    "accuracy=0\n",
    "weighted=0\n",
    "micro=0\n",
    "macro=0\n",
    "auc=0\n",
    "precision = 0\n",
    "recall = 0\n",
    "\n",
    "for i in range(0,10):\n",
    "    accuracy=accuracy+acc[i]\n",
    "    weighted=weighted+f1[i]\n",
    "    micro=micro+f2[i]\n",
    "    macro=macro+f3[i]\n",
    "    precision=precision+p[i]\n",
    "    recall=recall+r[i]\n",
    "    auc=auc+roc_auc[i]\n",
    "    \n",
    "accuracy=accuracy/10\n",
    "weighted=weighted/10\n",
    "micro=micro/10\n",
    "macro=macro/10\n",
    "auc=auc/10\n",
    "precision=precision/10\n",
    "recall=recall/10\n",
    "\n",
    "#accuracy\n",
    "print(\"{:.12f}\".format(accuracy))\n",
    "row1.append((\"{:.12f}\".format(accuracy)))\n",
    "#precision\n",
    "print(\"{:.12f}\".format(precision))\n",
    "row2.append((\"{:.12f}\".format(precision)))\n",
    "#recall\n",
    "print(\"{:.12f}\".format(recall))\n",
    "row3.append((\"{:.12f}\".format(recall)))\n",
    "#f-measure\n",
    "print(\"{:.12f}\".format(micro))\n",
    "row4.append((\"{:.12f}\".format(micro)))\n",
    "print(\"{:.12f}\".format(macro))\n",
    "row5.append((\"{:.12f}\".format(macro)))\n",
    "print(\"{:.12f}\".format(weighted))\n",
    "row6.append((\"{:.12f}\".format(weighted)))\n",
    "#auc\n",
    "#fpr, tpr, thresholds = metrics.roc_curve(y,pred,pos_label=2)\n",
    "#metrics.auc(fpr, tpr)\n",
    "print(\"{:.12f}\".format(auc))\n",
    "row7.append((\"{:.12f}\".format(auc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.789367409647\n",
      "0.367500000000\n",
      "0.053333092403\n",
      "0.789367409647\n",
      "0.482844062059\n",
      "0.716821327333\n",
      "0.617270930967\n"
     ]
    }
   ],
   "source": [
    "#RandomForestClassifier\n",
    "acc = []\n",
    "f1 = []\n",
    "f2 = []\n",
    "f3 = []\n",
    "roc_auc = []\n",
    "p =[]\n",
    "r = []\n",
    "\n",
    "kf = KFold(n_splits = 10)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "#     print(\" %s %s \" % (train_index, test_index))\n",
    "\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred = np.array(rfc.predict(X_test)) #y_score\n",
    "    y_score = np.array(rfc.predict_proba(X_test)[:,1])\n",
    "    y_test = np.array(y_test) #y_true\n",
    "    \n",
    "    acc.append(rfc.score(X_test,y_test))\n",
    "    f1.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    f2.append(f1_score(y_test, y_pred, average='micro'))\n",
    "    f3.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    roc_auc.append(roc_auc_score(y_test, y_score))\n",
    "    p.append(precision_score(y_test, y_pred))\n",
    "    r.append(recall_score(y_test, y_pred))\n",
    "accuracy=0\n",
    "weighted=0\n",
    "micro=0\n",
    "macro=0\n",
    "auc=0\n",
    "precision = 0\n",
    "recall = 0\n",
    "\n",
    "for i in range(0,10):\n",
    "    accuracy=accuracy+acc[i]\n",
    "    weighted=weighted+f1[i]\n",
    "    micro=micro+f2[i]\n",
    "    macro=macro+f3[i]\n",
    "    precision=precision+p[i]\n",
    "    recall=recall+r[i]\n",
    "    auc=auc+roc_auc[i]   \n",
    "\n",
    "accuracy=accuracy/10\n",
    "weighted=weighted/10\n",
    "micro=micro/10\n",
    "macro=macro/10\n",
    "auc=auc/10\n",
    "precision=precision/10\n",
    "recall=recall/10\n",
    "\n",
    "#accuracy\n",
    "print(\"{:.12f}\".format(accuracy))\n",
    "row1.append((\"{:.12f}\".format(accuracy)))\n",
    "#precision\n",
    "print(\"{:.12f}\".format(precision))\n",
    "row2.append((\"{:.12f}\".format(precision)))\n",
    "#recall\n",
    "print(\"{:.12f}\".format(recall))\n",
    "row3.append((\"{:.12f}\".format(recall)))\n",
    "#f-measure\n",
    "print(\"{:.12f}\".format(micro))\n",
    "row4.append((\"{:.12f}\".format(micro)))\n",
    "print(\"{:.12f}\".format(macro))\n",
    "row5.append((\"{:.12f}\".format(macro)))\n",
    "print(\"{:.12f}\".format(weighted))\n",
    "row6.append((\"{:.12f}\".format(weighted)))\n",
    "#auc\n",
    "#fpr, tpr, thresholds = metrics.roc_curve(y,pred,pos_label=2)\n",
    "#metrics.auc(fpr, tpr)\n",
    "print(\"{:.12f}\".format(auc))\n",
    "row7.append((\"{:.12f}\".format(auc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738630932059\n",
      "0.285250578174\n",
      "0.153115897866\n",
      "0.738630932059\n",
      "0.513756797084\n",
      "0.718339945243\n",
      "0.526006564685\n"
     ]
    }
   ],
   "source": [
    "#KNeighborsClassifier\n",
    "acc = []\n",
    "f1 = []\n",
    "f2 = []\n",
    "f3 = []\n",
    "roc_auc = []\n",
    "p =[]\n",
    "r = []\n",
    "\n",
    "kf = KFold(n_splits = 10)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "#     print(\" %s %s \" % (train_index, test_index))\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred = np.array(knn.predict(X_test)) #y_score\n",
    "    y_score = np.array(knn.predict_proba(X_test)[:,1])\n",
    "    y_test = np.array(y_test) #y_true\n",
    "    \n",
    "    acc.append(knn.score(X_test,y_test))\n",
    "    f1.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    f2.append(f1_score(y_test, y_pred, average='micro'))\n",
    "    f3.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    roc_auc.append(roc_auc_score(y_test, y_score))\n",
    "    p.append(precision_score(y_test, y_pred))\n",
    "    r.append(recall_score(y_test, y_pred))\n",
    "accuracy=0\n",
    "weighted=0\n",
    "micro=0\n",
    "macro=0\n",
    "auc=0\n",
    "precision = 0\n",
    "recall = 0\n",
    "\n",
    "for i in range(0,10):\n",
    "    accuracy=accuracy+acc[i]\n",
    "    weighted=weighted+f1[i]\n",
    "    micro=micro+f2[i]\n",
    "    macro=macro+f3[i]\n",
    "    precision=precision+p[i]\n",
    "    recall=recall+r[i]\n",
    "    auc=auc+roc_auc[i]\n",
    "    \n",
    "accuracy=accuracy/10\n",
    "weighted=weighted/10\n",
    "micro=micro/10\n",
    "macro=macro/10\n",
    "auc=auc/10\n",
    "precision=precision/10\n",
    "recall=recall/10\n",
    "\n",
    "#accuracy\n",
    "print(\"{:.12f}\".format(accuracy))\n",
    "row1.append((\"{:.12f}\".format(accuracy)))\n",
    "#precision\n",
    "print(\"{:.12f}\".format(precision))\n",
    "row2.append((\"{:.12f}\".format(precision)))\n",
    "#recall\n",
    "print(\"{:.12f}\".format(recall))\n",
    "row3.append((\"{:.12f}\".format(recall)))\n",
    "#f-measure\n",
    "print(\"{:.12f}\".format(micro))\n",
    "row4.append((\"{:.12f}\".format(micro)))\n",
    "print(\"{:.12f}\".format(macro))\n",
    "row5.append((\"{:.12f}\".format(macro)))\n",
    "print(\"{:.12f}\".format(weighted))\n",
    "row6.append((\"{:.12f}\".format(weighted)))\n",
    "#auc\n",
    "#fpr, tpr, thresholds = metrics.roc_curve(y,pred,pos_label=2)\n",
    "#metrics.auc(fpr, tpr)\n",
    "print(\"{:.12f}\".format(auc))\n",
    "row7.append((\"{:.12f}\".format(auc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.770660391162\n",
      "0.438838461587\n",
      "0.492216362703\n",
      "0.770660391162\n",
      "0.633636250569\n",
      "0.769174425139\n",
      "0.746577640268\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "acc = []\n",
    "f1 = []\n",
    "f2 = []\n",
    "f3 = []\n",
    "roc_auc = []\n",
    "p =[]\n",
    "r = []\n",
    "\n",
    "kf = KFold(n_splits = 10)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "#     print(\" %s %s \" % (train_index, test_index))\n",
    "    \n",
    "    sv = svm.SVC(kernel='linear')\n",
    "    sv.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred = np.array(sv.predict(X_test)) #y_score\n",
    "    y_score = np.array(sv.decision_function(X_test))\n",
    "    y_test = np.array(y_test) #y_true\n",
    "    \n",
    "    acc.append(sv.score(X_test,y_test))\n",
    "    f1.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    f2.append(f1_score(y_test, y_pred, average='micro'))\n",
    "    f3.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    roc_auc.append(roc_auc_score(y_test, y_score))\n",
    "    p.append(precision_score(y_test, y_pred))\n",
    "    r.append(recall_score(y_test, y_pred))\n",
    "accuracy=0\n",
    "weighted=0\n",
    "micro=0\n",
    "macro=0\n",
    "auc=0\n",
    "precision = 0\n",
    "recall = 0\n",
    "\n",
    "for i in range(0,10):\n",
    "    accuracy=accuracy+acc[i]\n",
    "    weighted=weighted+f1[i]\n",
    "    micro=micro+f2[i]\n",
    "    macro=macro+f3[i]\n",
    "    precision=precision+p[i]\n",
    "    recall=recall+r[i]\n",
    "    auc=auc+roc_auc[i]\n",
    "    \n",
    "accuracy=accuracy/10\n",
    "weighted=weighted/10\n",
    "micro=micro/10\n",
    "macro=macro/10\n",
    "auc=auc/10\n",
    "precision=precision/10\n",
    "recall=recall/10\n",
    "\n",
    "#accuracy\n",
    "print(\"{:.12f}\".format(accuracy))\n",
    "row1.append((\"{:.12f}\".format(accuracy)))\n",
    "#precision\n",
    "print(\"{:.12f}\".format(precision))\n",
    "row2.append((\"{:.12f}\".format(precision)))\n",
    "#recall\n",
    "print(\"{:.12f}\".format(recall))\n",
    "row3.append((\"{:.12f}\".format(recall)))\n",
    "#f-measure\n",
    "print(\"{:.12f}\".format(micro))\n",
    "row4.append((\"{:.12f}\".format(micro)))\n",
    "print(\"{:.12f}\".format(macro))\n",
    "row5.append((\"{:.12f}\".format(macro)))\n",
    "print(\"{:.12f}\".format(weighted))\n",
    "row6.append((\"{:.12f}\".format(weighted)))\n",
    "#auc\n",
    "#fpr, tpr, thresholds = metrics.roc_curve(y,pred,pos_label=2)\n",
    "#metrics.auc(fpr, tpr)\n",
    "print(\"{:.12f}\".format(auc))\n",
    "row7.append((\"{:.12f}\".format(auc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.752894698337\n",
      "0.394066553255\n",
      "0.425689378294\n",
      "0.752894698337\n",
      "0.598686465658\n",
      "0.750094411480\n",
      "0.643434185152\n"
     ]
    }
   ],
   "source": [
    "#DecisionTreeClassifier\n",
    "acc = []\n",
    "f1 = []\n",
    "f2 = []\n",
    "f3 = []\n",
    "roc_auc = []\n",
    "p =[]\n",
    "r = []\n",
    "\n",
    "kf = KFold(n_splits = 10)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "#     print(\" %s %s \" % (train_index, test_index))\n",
    "    \n",
    "    dtc = DecisionTreeClassifier() \n",
    "    dtc.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred = np.array(dtc.predict(X_test)) #y_score\n",
    "    y_score = np.array(dtc.predict_proba(X_test)[:,1])\n",
    "    y_test = np.array(y_test) #y_true\n",
    "    \n",
    "    acc.append(dtc.score(X_test,y_test))\n",
    "    f1.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    f2.append(f1_score(y_test, y_pred, average='micro'))\n",
    "    f3.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    roc_auc.append(roc_auc_score(y_test, y_score))\n",
    "    p.append(precision_score(y_test, y_pred))\n",
    "    r.append(recall_score(y_test, y_pred))\n",
    "accuracy=0\n",
    "weighted=0\n",
    "micro=0\n",
    "macro=0\n",
    "auc=0\n",
    "precision = 0\n",
    "recall = 0\n",
    "\n",
    "for i in range(0,10):\n",
    "    accuracy=accuracy+acc[i]\n",
    "    weighted=weighted+f1[i]\n",
    "    micro=micro+f2[i]\n",
    "    macro=macro+f3[i]\n",
    "    precision=precision+p[i]\n",
    "    recall=recall+r[i]\n",
    "    auc=auc+roc_auc[i]\n",
    "    \n",
    "accuracy=accuracy/10\n",
    "weighted=weighted/10\n",
    "micro=micro/10\n",
    "macro=macro/10\n",
    "auc=auc/10\n",
    "precision=precision/10\n",
    "recall=recall/10\n",
    "\n",
    "#accuracy\n",
    "print(\"{:.12f}\".format(accuracy))\n",
    "row1.append((\"{:.12f}\".format(accuracy)))\n",
    "#precision\n",
    "print(\"{:.12f}\".format(precision))\n",
    "row2.append((\"{:.12f}\".format(precision)))\n",
    "#recall\n",
    "print(\"{:.12f}\".format(recall))\n",
    "row3.append((\"{:.12f}\".format(recall)))\n",
    "#f-measure\n",
    "print(\"{:.12f}\".format(micro))\n",
    "row4.append((\"{:.12f}\".format(micro)))\n",
    "print(\"{:.12f}\".format(macro))\n",
    "row5.append((\"{:.12f}\".format(macro)))\n",
    "print(\"{:.12f}\".format(weighted))\n",
    "row6.append((\"{:.12f}\".format(weighted)))\n",
    "#auc\n",
    "#fpr, tpr, thresholds = metrics.roc_curve(y,pred,pos_label=2)\n",
    "#metrics.auc(fpr, tpr)\n",
    "print(\"{:.12f}\".format(auc))\n",
    "row7.append((\"{:.12f}\".format(auc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.665095351900\n",
      "0.255309859664\n",
      "0.338601500611\n",
      "0.665095351900\n",
      "0.513689592836\n",
      "0.683218605716\n",
      "0.548949191567\n"
     ]
    }
   ],
   "source": [
    "#GaussianNB\n",
    "acc = []\n",
    "f1 = []\n",
    "f2 = []\n",
    "f3 = []\n",
    "roc_auc = []\n",
    "p =[]\n",
    "r = []\n",
    "\n",
    "kf = KFold(n_splits = 10)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "    y_train, y_test = np.array(y)[train_index], np.array(y)[test_index]\n",
    "#     print(\" %s %s \" % (train_index, test_index))\n",
    "    \n",
    "    gnb = GaussianNB() \n",
    "    gnb.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred = np.array(gnb.predict(X_test)) #y_score\n",
    "    y_score = np.array(gnb.predict_proba(X_test)[:,1])\n",
    "    y_test = np.array(y_test) #y_true\n",
    "    \n",
    "    acc.append(gnb.score(X_test,y_test))\n",
    "    f1.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    f2.append(f1_score(y_test, y_pred, average='micro'))\n",
    "    f3.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    roc_auc.append(roc_auc_score(y_test, y_score))\n",
    "    p.append(precision_score(y_test, y_pred))\n",
    "    r.append(recall_score(y_test, y_pred))\n",
    "accuracy=0\n",
    "weighted=0\n",
    "micro=0\n",
    "macro=0\n",
    "auc=0\n",
    "precision = 0\n",
    "recall = 0\n",
    "\n",
    "for i in range(0,10):\n",
    "    accuracy=accuracy+acc[i]\n",
    "    weighted=weighted+f1[i]\n",
    "    micro=micro+f2[i]\n",
    "    macro=macro+f3[i]\n",
    "    precision=precision+p[i]\n",
    "    recall=recall+r[i]\n",
    "    auc=auc+roc_auc[i]\n",
    "    \n",
    "accuracy=accuracy/10\n",
    "weighted=weighted/10\n",
    "micro=micro/10\n",
    "macro=macro/10\n",
    "auc=auc/10\n",
    "precision=precision/10\n",
    "recall=recall/10\n",
    "\n",
    "#accuracy\n",
    "print(\"{:.12f}\".format(accuracy))\n",
    "row1.append((\"{:.12f}\".format(accuracy)))\n",
    "#precision\n",
    "print(\"{:.12f}\".format(precision))\n",
    "row2.append((\"{:.12f}\".format(precision)))\n",
    "#recall\n",
    "print(\"{:.12f}\".format(recall))\n",
    "row3.append((\"{:.12f}\".format(recall)))\n",
    "#f-measure\n",
    "print(\"{:.12f}\".format(micro))\n",
    "row4.append((\"{:.12f}\".format(micro)))\n",
    "print(\"{:.12f}\".format(macro))\n",
    "row5.append((\"{:.12f}\".format(macro)))\n",
    "print(\"{:.12f}\".format(weighted))\n",
    "row6.append((\"{:.12f}\".format(weighted)))\n",
    "#auc\n",
    "#fpr, tpr, thresholds = metrics.roc_curve(y,pred,pos_label=2)\n",
    "#metrics.auc(fpr, tpr)\n",
    "print(\"{:.12f}\".format(auc))\n",
    "row7.append((\"{:.12f}\".format(auc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('OrignalData/Model_all/metricsall_MS4.csv', 'a', encoding='UTF-8', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(title)\n",
    "    writer.writerow(row1)\n",
    "    writer.writerow(row2)\n",
    "    writer.writerow(row3)\n",
    "    writer.writerow(row4)\n",
    "    writer.writerow(row5)\n",
    "    writer.writerow(row6)\n",
    "    writer.writerow(row7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
